{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbc112-c2ff-497a-8401-00838d15f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.        from_pretrained (\"openai-community/openai-gpt\")\n",
    "model     = transformers.AutoModelForCausalLM. from_pretrained (\"openai-community/openai-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88102143-4872-43d7-a48b-ac55d94ec5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = transformers.AutoImageProcessor.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n",
    "model     = transformers.AutoModelForImageClassification.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92683469-2fe7-403a-aa41-6c372a33f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# STEP 1 of PIPELINE\n",
    "tokenizer           = transformers.BertTokenizerFast    .from_pretrained(\"bert-base-cased\") # tokenizer = transformers. AutoTokenizer. from_pretrained(\"bert-base-cased\")\n",
    "raw_inputs = [\n",
    "    \"I love you\",    \n",
    "    \"I hate you\",            \n",
    "    \"I hate you, like I love you\",\n",
    "]\n",
    "input_numeric_ids = tokenizer(raw_inputs           , return_tensors= \"pt\", padding=True, truncation= False )\n",
    "\n",
    "# STEP 2 of PIPELINE\n",
    "config              = transformers.BertConfig()\n",
    "model_untrained     = transformers.BertModel(config)\n",
    "model_pretrained    = transformers.BertModel(config).from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model_general         = transformers. AutoModel.                         from_pretrained(\"bert-base-cased\")\n",
    "model_classification  = transformers. AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Understanding model's output\n",
    "outputs = model_pretrained(**input_numeric_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f614d-1200-4d9b-80be-9b035f0b0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers # Working on text\n",
    "import diffusers    # Working on images\n",
    "\n",
    "\n",
    "checkpoint  = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer   = transformers. AutoTokenizer.                       from_pretrained(checkpoint)\n",
    "model       = transformers. AutoModelForSequenceClassification.  from_pretrained(checkpoint)\n",
    "sequences   = [\n",
    "    \"I love you\",    \n",
    "    \"I hate you\",            \n",
    "    \"I hate you, like I love you\",\n",
    "]\n",
    "\n",
    "tokens      = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output      = model (   **tokens    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbba9b3-8710-42b3-aa66-0b14d79ee351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.pipeline(\"sentiment-analysis\", \n",
    "                 model_kwargs= {\"force_download\": True} \n",
    "                )\n",
    "model(\n",
    "    [\"I love you\", \"I hate you\",]\n",
    ")\n",
    "\n",
    "categorization = transformers.pipeline(\"zero-shot-classification\", \n",
    "                          model_kwargs= {\"force_download\": True}\n",
    "                        )\n",
    "categorization(\n",
    "    [\"Bio Tech Journal\", \"Nano Tech Journal\", \"Land dispute\"],\n",
    "    candidate_labels=[\"science\", \"law\", \"land law\"],\n",
    ")\n",
    "\n",
    "generator = transformers.pipeline(\"text-generation\")\n",
    "generator(\"Assistant for science textbook writers\")\n",
    "\n",
    "metadata_extractor = transformers.pipeline(\"ner\", grouped_entities=True)\n",
    "metadata_extractor(\"<Header> Chapter Name. <p> Introduction to Large Language Models\")\n",
    "\n",
    "question_answerer = transformers.pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")\n",
    "\n",
    "summarizer = transformers.pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    this is a long sentence. which just talks about how long it is. That's it. \n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
